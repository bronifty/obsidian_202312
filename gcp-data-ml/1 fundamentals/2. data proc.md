pubsub dataflow looker

1 variety
2 volume
3 velocity
4 veracity

# pubsub

publisher (iot device) sends message to pubsub to:
	ingest
	read
	store
	broadcast

subscriber (dataflow) receives these messages to:
	ingest 
	transform 
	output

downstream bi tool (looker)
	visualize
	explore


- publisher : subscriber is m:m where messages are published to a topic, where subscribers receive them; the publisher and subscriber are decoupled by the topic
- pubsub buffers changes for lightly-coupled architectures

### apache beam

- pipe data into warehouse for analysis with dataflow
- dataflow pipes both batch and streaming data 
- apache beam is a pipeline design system for ETL batch and streaming
	- unifies batch and streaming
	- works on dataflow and apache spark
	- extensible (write your own connectors and transformation libs)
	- provides pipeline templates 
		- write pipelines in java, python or go
	- sdk provides a lib for a variety of transforms and connectors to sources and sinks
	- ETL pipelines in GCP are written with Apache Beam, which abstracts these workflows into a blueprint which is portable across runtimes like Dataflow/Spark/Flink/etc so you can either pick a template or write Go/Python pipelines and reuse them anywhere.

> Aside: I would characterize distributed data processing engines like dataflow or spark as tools for initial heavy lifting related to entire lakehouses or warehouses of data where standards are applied universally, whereas something like DBT is used for local transformations in-place which creates datamarts or silos

